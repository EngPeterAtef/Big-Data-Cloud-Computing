{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOfhYu5WR7j",
        "outputId": "33e5698b-35ca-487e-956e-ec15f2631985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJEMS45iWKEO",
        "outputId": "bc85eaa8-ea42-4072-ce19-36ab6b75736d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Big Data Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7vPTp1WIWPlt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import col, countDistinct, isnan, struct\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, VectorIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator,ClusteringEvaluator\n",
        "from tabulate import tabulate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RhfzwjH7WZXn"
      },
      "outputs": [],
      "source": [
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"LoanApproval\")\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Gg7OCwzgWWTJ"
      },
      "outputs": [],
      "source": [
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J6p0NgA2kNb",
        "outputId": "fa874e21-efa8-4aff-ecd9-8a14ae08d383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- State: string (nullable = true)\n",
            " |-- Bank: string (nullable = true)\n",
            " |-- BankState: string (nullable = true)\n",
            " |-- Term: integer (nullable = true)\n",
            " |-- NoEmp: integer (nullable = true)\n",
            " |-- NewExist: integer (nullable = true)\n",
            " |-- CreateJob: integer (nullable = true)\n",
            " |-- RetainedJob: integer (nullable = true)\n",
            " |-- UrbanRural: integer (nullable = true)\n",
            " |-- RevLineCr: integer (nullable = true)\n",
            " |-- LowDoc: integer (nullable = true)\n",
            " |-- Sector: integer (nullable = true)\n",
            " |-- ApprovalMonth: string (nullable = true)\n",
            " |-- IsFranchise: integer (nullable = true)\n",
            " |-- clean_DisbursementGross: double (nullable = true)\n",
            " |-- MIS_Status: integer (nullable = true)\n",
            " |-- clean_ChgOffPrinGr: double (nullable = true)\n",
            " |-- clean_GrAppv: double (nullable = true)\n",
            " |-- clean_SBA_Appv: double (nullable = true)\n",
            "\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+\n",
            "|State|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|Sector|ApprovalMonth|IsFranchise|clean_DisbursementGross|MIS_Status|clean_ChgOffPrinGr|clean_GrAppv|clean_SBA_Appv|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+\n",
            "|   MA|TD BANK, NATIONAL...|       DE|  84|    2|       0|        0|          2|         1|        1|     0|    44|          Mar|          0|                25959.0|         1|               0.0|     10000.0|        5000.0|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    7|       0|        0|          7|         1|        1|     0|    23|          Jun|          0|                98479.0|         1|               0.0|     50000.0|       25000.0|\n",
            "|   MA|FLORENCE SAVINGS ...|       MA|  60|    2|       0|        0|          2|         1|        1|     0|    23|          Apr|          0|               135070.0|         1|               0.0|     35000.0|       17500.0|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    4|       0|        0|          4|         1|        0|     0|    72|          Mar|          0|                20000.0|         1|               0.0|     20000.0|       10000.0|\n",
            "|   MA|BANK OF AMERICA N...|       MA|  84|    6|       1|        0|          0|         0|        0|     1|    81|          Apr|          0|                50000.0|         1|               0.0|     50000.0|       45000.0|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_path=\"./1000.csv\"\n",
        "loan_df =  spark.read.csv(data_path, header=True, inferSchema=True, multiLine=True, quote='\"', escape='\"')\n",
        "loan_df = loan_df.drop('Name')\n",
        "loan_df = loan_df.drop('Zip')\n",
        "loan_df = loan_df.drop('City')\n",
        "loan_df.printSchema()\n",
        "loan_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah6-76fa5Y7_",
        "outputId": "a1f7bb4a-dc73-43ad-e156-27f1c9b1b011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming categorial features...\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+----------+---------+--------------+---------------+-----------+------------------+-------------+-----------------+--------------+-------------+--------------+----------------+--------------------+\n",
            "|State|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|Sector|ApprovalMonth|IsFranchise|clean_DisbursementGross|MIS_Status|clean_ChgOffPrinGr|clean_GrAppv|clean_SBA_Appv|StateIndex|BankIndex|BankStateIndex|UrbanRuralIndex|SectorIndex|ApprovalMonthIndex|     StateVec|          BankVec|  BankStateVec|UrbanRuralVec|     SectorVec|ApprovalMonthVec|            features|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+----------+---------+--------------+---------------+-----------+------------------+-------------+-----------------+--------------+-------------+--------------+----------------+--------------------+\n",
            "|   MA|TD BANK, NATIONAL...|       DE|  84|    2|       0|        0|          2|         1|        1|     0|    44|          Mar|          0|                25959.0|         1|               0.0|     10000.0|        5000.0|       0.0|      2.0|           5.0|            0.0|        1.0|               3.0|(9,[0],[1.0])|  (145,[2],[1.0])|(22,[5],[1.0])|(3,[0],[1.0])|(18,[1],[1.0])|  (12,[3],[1.0])|(221,[0,1,4,5,8,1...|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    7|       0|        0|          7|         1|        1|     0|    23|          Jun|          0|                98479.0|         1|               0.0|     50000.0|       25000.0|       0.0|      0.0|           0.0|            0.0|        2.0|               0.0|(9,[0],[1.0])|  (145,[0],[1.0])|(22,[0],[1.0])|(3,[0],[1.0])|(18,[2],[1.0])|  (12,[0],[1.0])|(221,[0,1,4,5,8,1...|\n",
            "|   MA|FLORENCE SAVINGS ...|       MA|  60|    2|       0|        0|          2|         1|        1|     0|    23|          Apr|          0|               135070.0|         1|               0.0|     35000.0|       17500.0|       0.0|    104.0|           1.0|            0.0|        2.0|               1.0|(9,[0],[1.0])|(145,[104],[1.0])|(22,[1],[1.0])|(3,[0],[1.0])|(18,[2],[1.0])|  (12,[1],[1.0])|(221,[0,1,4,5,8,1...|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    4|       0|        0|          4|         1|        0|     0|    72|          Mar|          0|                20000.0|         1|               0.0|     20000.0|       10000.0|       0.0|      0.0|           0.0|            0.0|        6.0|               3.0|(9,[0],[1.0])|  (145,[0],[1.0])|(22,[0],[1.0])|(3,[0],[1.0])|(18,[6],[1.0])|  (12,[3],[1.0])|(221,[0,1,4,8,10,...|\n",
            "|   MA|BANK OF AMERICA N...|       MA|  84|    6|       1|        0|          0|         0|        0|     1|    81|          Apr|          0|                50000.0|         1|               0.0|     50000.0|       45000.0|       0.0|      1.0|           1.0|            1.0|        3.0|               1.0|(9,[0],[1.0])|  (145,[1],[1.0])|(22,[1],[1.0])|(3,[1],[1.0])|(18,[3],[1.0])|  (12,[1],[1.0])|(221,[0,1,2,6,8,1...|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+------+-------------+-----------+-----------------------+----------+------------------+------------+--------------+----------+---------+--------------+---------------+-----------+------------------+-------------+-----------------+--------------+-------------+--------------+----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Splitting data into training and test...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Transforming categorial features...\")\n",
        "# List of categorical columns to be one-hot encoded\n",
        "\n",
        "\n",
        "categorical_columns = [\"State\", \"Bank\", \"BankState\", \"UrbanRural\", \"Sector\", \"ApprovalMonth\"]\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "# Define an empty list to store the pipeline stages\n",
        "stages = []\n",
        "\n",
        "# Fit StringIndexer on the entire dataset and add to stages\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column + \"Index\").fit(loan_df) for column in categorical_columns]\n",
        "stages += indexers\n",
        "\n",
        "# Define OneHotEncoder for the indexed columns\n",
        "encoders = [OneHotEncoder(inputCol=column + \"Index\", outputCol=column + \"Vec\", dropLast=False) for column in categorical_columns]\n",
        "stages += encoders\n",
        "\n",
        "\n",
        "label_column = \"MIS_Status\"\n",
        "\n",
        "# Create VectorAssembler for combining all features\n",
        "# List of input columns (excluding the label column and categorical columns)\n",
        "input_columns = [col for col in loan_df.columns if col != label_column and col not in categorical_columns]\n",
        "input_columns += [column + \"Vec\" for column in categorical_columns]\n",
        "assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
        "\n",
        "# Combine all stages into a Pipeline\n",
        "pipeline = Pipeline(stages=stages + [assembler])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "pipeline_model = pipeline.fit(loan_df)\n",
        "\n",
        "# Transform your data using the pipeline\n",
        "transformed_data = pipeline_model.transform(loan_df)\n",
        "transformed_data.show(5)\n",
        "\n",
        "print(\"Splitting data into training and test...\")\n",
        "(trainingData, testData) = transformed_data.randomSplit([0.8, 0.2], seed=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGK4OcjX-zg"
      },
      "source": [
        "## Make a new DF that has only MIS_Status and features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeQn5Pm753VE",
        "outputId": "9e70612a-5ece-4368-9e33-1e2c01117dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------------------------------------------------------------------------------------------------------------------+\n",
            "|MIS_Status|features                                                                                                                        |\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1         |(221,[0,1,4,8,10,11,19,66,168,190,207,214],[36.0,3.0,3.0,20000.0,20000.0,10000.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
            "|1         |(221,[0,1,6,8,10,11,14,104,169,189,191,213],[84.0,4.0,1.0,30000.0,30000.0,27000.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |\n",
            "|1         |(221,[0,3,8,10,11,14,79,178,188,202,215],[84.0,24.0,30000.0,30000.0,15000.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
            "|0         |(221,[0,1,4,5,8,9,10,11,14,22,170,188,194,211],[5.0,4.0,4.0,1.0,20158.0,1540.0,10000.0,5000.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
            "|0         |(221,[0,1,3,4,5,8,9,10,11,14,22,170,188,192,219],[25.0,6.0,1.0,7.0,1.0,79463.0,50000.0,50000.0,25000.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_df = trainingData.select(\"MIS_Status\", \"features\")\n",
        "test_df = testData.select(\"MIS_Status\", \"features\")\n",
        "training_df.show(5,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nsm8sdZYHV2"
      },
      "source": [
        "## Convert to RDD to Apply MapReduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAXKR0KA6rXo",
        "outputId": "cc667fdf-98d5-497c-abc3-45b6dbd46104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(MIS_Status=1, features=SparseVector(221, {0: 36.0, 1: 3.0, 4: 3.0, 8: 20000.0, 10: 20000.0, 11: 10000.0, 19: 1.0, 66: 1.0, 168: 1.0, 190: 1.0, 207: 1.0, 214: 1.0})), Row(MIS_Status=1, features=SparseVector(221, {0: 84.0, 1: 4.0, 6: 1.0, 8: 30000.0, 10: 30000.0, 11: 27000.0, 14: 1.0, 104: 1.0, 169: 1.0, 189: 1.0, 191: 1.0, 213: 1.0})), Row(MIS_Status=1, features=SparseVector(221, {0: 84.0, 3: 24.0, 8: 30000.0, 10: 30000.0, 11: 15000.0, 14: 1.0, 79: 1.0, 178: 1.0, 188: 1.0, 202: 1.0, 215: 1.0})), Row(MIS_Status=0, features=SparseVector(221, {0: 5.0, 1: 4.0, 4: 4.0, 5: 1.0, 8: 20158.0, 9: 1540.0, 10: 10000.0, 11: 5000.0, 14: 1.0, 22: 1.0, 170: 1.0, 188: 1.0, 194: 1.0, 211: 1.0})), Row(MIS_Status=0, features=SparseVector(221, {0: 25.0, 1: 6.0, 3: 1.0, 4: 7.0, 5: 1.0, 8: 79463.0, 9: 50000.0, 10: 50000.0, 11: 25000.0, 14: 1.0, 22: 1.0, 170: 1.0, 188: 1.0, 192: 1.0, 219: 1.0}))]\n"
          ]
        }
      ],
      "source": [
        "training_rdd = training_df.rdd\n",
        "test_rdd = test_df.rdd\n",
        "print(training_rdd.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k1jCNLL_nm7",
        "outputId": "1a82a99e-609e-4a2d-f63c-c5205a5f4cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions before repartitioning: 1\n",
            "Number of partitions after repartitioning: 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of partitions before repartitioning:\", training_rdd.getNumPartitions())\n",
        "# Repartition the RDD into a new number of partitions\n",
        "num_partitions = 3  # Change this to the desired number of partitions\n",
        "training_rdd = training_rdd.repartition(num_partitions)\n",
        "# New number of partitions\n",
        "print(\"Number of partitions after repartitioning:\", training_rdd.getNumPartitions())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWLIV0vGXg7R"
      },
      "outputs": [],
      "source": [
        "# Collect the elements of the RDD into a list\n",
        "test_list = test_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZwyDGbXk6f"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WwIh7JLp9Spq"
      },
      "outputs": [],
      "source": [
        "def appy_knn(rdd, query_point, k):\n",
        "  def cosine_similarity(np_vector1, np_vector2):\n",
        "    # Compute dot product\n",
        "    dot_product = np.dot(np_vector1, np_vector2)\n",
        "\n",
        "    # Compute magnitudes\n",
        "    mag1 = np.sqrt(np.sum(np_vector1 ** 2))\n",
        "    mag2 = np.sqrt(np.sum(np_vector2 ** 2))\n",
        "\n",
        "    # Handle division by zero\n",
        "    if mag1 == 0 or mag2 == 0:\n",
        "        return 0\n",
        "    # Compute cosine similarity\n",
        "    return dot_product / (mag1 * mag2)\n",
        "\n",
        "  def map_phase(split):\n",
        "      \"\"\"Map phase: Find k-nearest neighbors in each split.\"\"\"\n",
        "      neighbors = []\n",
        "      for row in split:\n",
        "          true_class = row.MIS_Status\n",
        "          data_point = row.features\n",
        "          # Convert PySpark sparse vectors to NumPy arrays\n",
        "          np_vector1 = np.array(query_point.toArray())\n",
        "          np_vector2 = np.array(data_point.toArray())\n",
        "          # Calculate cosine similarity\n",
        "          dist = cosine_similarity(np_vector1, np_vector2)\n",
        "          neighbors.append((None, {'similarity': dist, 'class': true_class}))\n",
        "      # Sort the neighbors by similarity\n",
        "      neighbors.sort(key=lambda x: x[1]['similarity'], reverse=True)\n",
        "      # Take the top k neighbors\n",
        "      k_neighbors = neighbors[:k]\n",
        "\n",
        "      return [k_neighbors]\n",
        "\n",
        "  def reduce_phase(neighbors1, neighbors2):\n",
        "      \"\"\"Reduce phase: Find the definitive top k neighbors.\"\"\"\n",
        "      # Merge the neighbors from different splits\n",
        "      merged_neighbors = neighbors1 + neighbors2\n",
        "      # Sort the merged neighbors by distance\n",
        "      merged_neighbors.sort(key=lambda x: x[1]['similarity'], reverse=True)\n",
        "      # Take the top k neighbors\n",
        "      return merged_neighbors[:k]\n",
        "\n",
        "  def classify_input(data):\n",
        "    # Extract the classes from the data\n",
        "    classes = np.array([entry[1]['class'] for entry in data])\n",
        "\n",
        "    # Count the occurrences of each class\n",
        "    class_counts = np.bincount(classes)\n",
        "\n",
        "    # Find the most common class\n",
        "    most_common_class = np.argmax(class_counts)\n",
        "\n",
        "    # print(\"Most frequent class:\", most_common_class)\n",
        "    return most_common_class\n",
        "\n",
        "  # Map phase: Apply map transformation to each split of the training data\n",
        "  mapped_neighbors = rdd.mapPartitions(map_phase)\n",
        "  # print(\"mapped_neighbors_rdd\")\n",
        "  # print(\"Number of partitions:\", mapped_neighbors.getNumPartitions())\n",
        "  # print(mapped_neighbors.take(10))\n",
        "\n",
        "  # Reduce phase: Aggregate results from the map phase using reduce\n",
        "  final_neighbors = mapped_neighbors.reduce(reduce_phase)\n",
        "  # print(\"Final K Nearest Neighbors:\", final_neighbors)\n",
        "  return classify_input(final_neighbors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKmtLNmbXUAk"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bw9VS-N5LBMF"
      },
      "outputs": [],
      "source": [
        "def calculate_confusion_matrix(true_labels, predicted_labels, labels):\n",
        "    num_classes = len(labels)\n",
        "    confusion_matrix = [[0] * num_classes for _ in range(num_classes)]\n",
        "    label_to_index = {label: i for i, label in enumerate(labels)}\n",
        "    for true_label, predicted_label in zip(true_labels, predicted_labels):\n",
        "        true_index = label_to_index[true_label]\n",
        "        predicted_index = label_to_index[predicted_label]\n",
        "        confusion_matrix[true_index][predicted_index] += 1\n",
        "    return confusion_matrix\n",
        "\n",
        "\n",
        "def calculate_accuracy(confusion_matrix):\n",
        "    tp_tn = sum(confusion_matrix[i][i] for i in range(len(confusion_matrix)))\n",
        "    tp_tn_fp_fn = sum(sum(row) for row in confusion_matrix)\n",
        "    accuracy = tp_tn / tp_tn_fp_fn\n",
        "    return accuracy\n",
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "    return 2* (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_precision(confusion_matrix, class_index):\n",
        "    # Calculate precision for a specific class\n",
        "    true_positive = confusion_matrix[class_index][class_index]\n",
        "    column_sum = sum(confusion_matrix[i][class_index] for i in range(len(confusion_matrix)))\n",
        "    precision = true_positive / column_sum if column_sum != 0 else 0\n",
        "    return precision\n",
        "\n",
        "def calculate_recall(confusion_matrix, class_index):\n",
        "    # Calculate recall for a specific class\n",
        "    true_positive = confusion_matrix[class_index][class_index]\n",
        "    row_sum = sum(confusion_matrix[class_index])\n",
        "    recall = true_positive / row_sum if row_sum != 0 else 0\n",
        "    return recall\n",
        "\n",
        "def calculate_macro_average_precision(confusion_matrix):\n",
        "    num_classes = len(confusion_matrix)\n",
        "    precisions = [calculate_precision(confusion_matrix, i) for i in range(num_classes)]\n",
        "    macro_average_precision = sum(precisions) / num_classes\n",
        "    return macro_average_precision\n",
        "\n",
        "def calculate_macro_average_recall(confusion_matrix):\n",
        "    num_classes = len(confusion_matrix)\n",
        "    recalls = [calculate_recall(confusion_matrix, i) for i in range(num_classes)]\n",
        "    macro_average_recall = sum(recalls) / num_classes\n",
        "    return macro_average_recall\n",
        "\n",
        "\n",
        "def calculate_micro_average_precision(confusion_matrix):\n",
        "    num_classes = len(confusion_matrix)\n",
        "    true_positives = sum(confusion_matrix[i][i] for i in range(num_classes))\n",
        "    all_positives = sum(sum(confusion_matrix[i]) for i in range(num_classes))\n",
        "    micro_average_precision = true_positives / all_positives if all_positives != 0 else 0\n",
        "    return micro_average_precision\n",
        "\n",
        "def calculate_micro_average_recall(confusion_matrix):\n",
        "    num_classes = len(confusion_matrix)\n",
        "    true_positives = sum(confusion_matrix[i][i] for i in range(num_classes))\n",
        "    all_actuals = sum(sum(row) for row in confusion_matrix)\n",
        "    micro_average_recall = true_positives / all_actuals if all_actuals != 0 else 0\n",
        "    return micro_average_recall\n",
        "def display_confusion_matrix(confusion_matrix, labels):\n",
        "     # Prepare data for tabulate\n",
        "    table = [\n",
        "        ['', *labels],\n",
        "        [labels[0],*confusion_matrix[0]],\n",
        "        [labels[1],*confusion_matrix[1]]\n",
        "    ]\n",
        "    # Display results using tabulate\n",
        "    print(tabulate(table, headers=\"firstrow\", tablefmt='grid'))\n",
        "\n",
        "def evaluate_knn(test_list):\n",
        "  predicted_list = []\n",
        "  true_list = []\n",
        "  for row in test_list:\n",
        "    true_label = row.MIS_Status\n",
        "    point = row.features\n",
        "    knn_predict = appy_knn(training_rdd, point, 3)\n",
        "    predicted_list.append(knn_predict)\n",
        "    true_list.append(true_label)\n",
        "  # print(true_label)\n",
        "  labels = [1,0]\n",
        "  # Calculate accuracy manually\n",
        "  confusion_matrix= calculate_confusion_matrix(true_list, predicted_list, labels=labels)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  display_confusion_matrix(confusion_matrix, labels)\n",
        "  accuracy = calculate_accuracy(confusion_matrix)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "  macro_precision = calculate_macro_average_precision(confusion_matrix)\n",
        "  print(\"macro_precision:\", macro_precision)\n",
        "  micro_precision = calculate_micro_average_precision(confusion_matrix)\n",
        "  print(\"micro_precision:\", micro_precision)\n",
        "\n",
        "  macro_recall = calculate_macro_average_recall(confusion_matrix)\n",
        "  print(\"macro_recall:\", macro_recall)\n",
        "  micro_recall = calculate_micro_average_recall(confusion_matrix)\n",
        "  print(\"micro_recall:\", micro_recall)\n",
        "\n",
        "  f1_macro = calculate_f1_score(macro_precision,macro_recall)\n",
        "  print(\"f1_macro:\", f1_macro)\n",
        "  f1_micro = calculate_f1_score(micro_precision,micro_recall)\n",
        "  print(\"f1_micro:\", f1_micro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJiSuP26V7tO",
        "outputId": "71d4bff8-f8a6-4b69-cfdf-2d6feedaa5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "+----+-----+-----+\n",
            "|    |   1 |   0 |\n",
            "+====+=====+=====+\n",
            "|  1 | 189 |   1 |\n",
            "+----+-----+-----+\n",
            "|  0 |   0 |  18 |\n",
            "+----+-----+-----+\n",
            "Accuracy: 0.9951923076923077\n",
            "macro_precision: 0.9736842105263157\n",
            "micro_precision: 0.9951923076923077\n",
            "macro_recall: 0.9973684210526316\n",
            "micro_recall: 0.9951923076923077\n",
            "f1_macro: 0.9853840207996628\n",
            "f1_micro: 0.9951923076923077\n"
          ]
        }
      ],
      "source": [
        "evaluate_knn(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "nX2VxEkkW10y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
