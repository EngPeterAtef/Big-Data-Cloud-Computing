{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chbic3fo9AJR"
      },
      "source": [
        " ## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6Ayhfq9BvN",
        "outputId": "8b94f648-0efa-40aa-d8c9-c3e1c20d931e"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfpbvgavADeB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7LveuaXi9AJS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import col, countDistinct, isnan, when, count, round, substring_index,substring, split, regexp_replace, udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, VectorIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator,ClusteringEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, LinearSVC,GBTClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALGQJI-b9QTx",
        "outputId": "a27f33ee-1260-4aa9-c4f8-13af0012183c"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcITIKfJAJ9G"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtFnxOG2AMkv"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_data, validation_data, test_data, model_name, label_column):\n",
        "    # Evaluate training data\n",
        "    train_predictions = model.transform(train_data)\n",
        "    train_metrics = calculate_metrics(train_predictions, label_column)\n",
        "\n",
        "    # Evaluate validation data\n",
        "    validation_predictions = model.transform(validation_data)\n",
        "    validation_metrics = calculate_metrics(validation_predictions, label_column)\n",
        "\n",
        "    # Evaluate test data\n",
        "    test_predictions = model.transform(test_data)\n",
        "    test_metrics = calculate_metrics(test_predictions, label_column)\n",
        "\n",
        "     # Prepare data for tabulate\n",
        "    table = [\n",
        "        ['Model', model_name,'',''],\n",
        "        ['Metric', 'Training', 'Validation', 'Test'],\n",
        "        ['Accuracy', train_metrics['accuracy'], validation_metrics['accuracy'], test_metrics['accuracy']],\n",
        "        ['Weighted Precision', train_metrics['weighted_precision'], validation_metrics['weighted_precision'], test_metrics['weighted_precision']],\n",
        "        ['Weighted Recall', train_metrics['weighted_recall'], validation_metrics['weighted_recall'], test_metrics['weighted_recall']],\n",
        "        ['F1 Score', train_metrics['f1'], validation_metrics['f1'], test_metrics['f1']]\n",
        "    ]\n",
        "\n",
        "    # Display results using tabulate\n",
        "    print(tabulate(table, headers=\"firstrow\", tablefmt='grid'))\n",
        "\n",
        "def calculate_metrics(predictions, label_column):\n",
        "    evaluator_multi = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='accuracy')\n",
        "    evaluator_weighted_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='weightedPrecision')\n",
        "    evaluator_weighted_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='weightedRecall')\n",
        "    evaluator_f1 = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='f1')\n",
        "\n",
        "    accuracy = evaluator_multi.evaluate(predictions)\n",
        "    weighted_precision = evaluator_weighted_precision.evaluate(predictions)\n",
        "    weighted_recall = evaluator_weighted_recall.evaluate(predictions)\n",
        "    f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'weighted_precision': weighted_precision,\n",
        "        'weighted_recall': weighted_recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OcvZDw5A9AJT"
      },
      "outputs": [],
      "source": [
        "\n",
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"LoanApproval\")\\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xxihZQnF9AJT"
      },
      "outputs": [],
      "source": [
        "\n",
        "sc=spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2HnF7W69AJU"
      },
      "source": [
        " ## Read Data - SBAnational.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1HbGwdzc9AJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# data_path=\"/content/drive/MyDrive/Colab Notebooks/BD_project/50000.csv\"\n",
        "data_path=\"../sample_data/50000.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JjluWYjC9AJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "loan_df =  spark.read.csv(data_path, header=True, inferSchema=True, multiLine=True, quote='\"', escape='\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkDD_RBf9AJU",
        "outputId": "b8665171-c592-4229-a752-3348289ba566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Zip: integer (nullable = true)\n",
            " |-- Bank: string (nullable = true)\n",
            " |-- BankState: string (nullable = true)\n",
            " |-- Term: integer (nullable = true)\n",
            " |-- NoEmp: integer (nullable = true)\n",
            " |-- NewExist: integer (nullable = true)\n",
            " |-- CreateJob: integer (nullable = true)\n",
            " |-- RetainedJob: integer (nullable = true)\n",
            " |-- UrbanRural: integer (nullable = true)\n",
            " |-- RevLineCr: integer (nullable = true)\n",
            " |-- LowDoc: integer (nullable = true)\n",
            " |-- MIS_Status: integer (nullable = true)\n",
            " |-- Sector: integer (nullable = true)\n",
            " |-- ApprovalMonth: string (nullable = true)\n",
            " |-- IsFranchise: integer (nullable = true)\n",
            " |-- clean_DisbursementGross: double (nullable = true)\n",
            " |-- clean_ChgOffPrinGr: double (nullable = true)\n",
            " |-- clean_GrAppv: double (nullable = true)\n",
            " |-- clean_SBA_Appv: double (nullable = true)\n",
            "\n",
            "+--------------------+--------------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+\n",
            "|                Name|          City|State| Zip|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|MIS_Status|Sector|ApprovalMonth|IsFranchise|clean_DisbursementGross|clean_ChgOffPrinGr|clean_GrAppv|clean_SBA_Appv|\n",
            "+--------------------+--------------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+\n",
            "|EMANUEL CHRISTIAN...|       HOLYOKE|   MA|1040|TD BANK, NATIONAL...|       DE|  84|    2|       0|        0|          2|         1|        1|     0|         1|    44|          Mar|          0|                25959.0|               0.0|     10000.0|        5000.0|\n",
            "|JA PATENAUDE CO L...|        MONSON|   MA|1057|CITIZENS BANK NAT...|       RI|  84|    7|       0|        0|          7|         1|        1|     0|         1|    23|          Jun|          0|                98479.0|               0.0|     50000.0|       25000.0|\n",
            "|    JC & COMPANY INC|   NORTHAMPTON|   MA|1060|FLORENCE SAVINGS ...|       MA|  60|    2|       0|        0|          2|         1|        1|     0|         1|    23|          Apr|          0|               135070.0|               0.0|     35000.0|       17500.0|\n",
            "|BUGRA, LLC DBA VA...|     WESTFIELD|   MA|1085|CITIZENS BANK NAT...|       RI|  84|    4|       0|        0|          4|         1|        0|     0|         1|    72|          Mar|          0|                20000.0|               0.0|     20000.0|       10000.0|\n",
            "|WESTERN MASS AUTO...|W. SPRINGFIELD|   MA|1089|BANK OF AMERICA N...|       MA|  84|    6|       1|        0|          0|         0|        0|     1|         1|    81|          Apr|          0|                50000.0|               0.0|     50000.0|       45000.0|\n",
            "+--------------------+--------------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loan_df.printSchema()\n",
        "loan_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPp_5zgk9AJU",
        "outputId": "b20f8619-2703-4c4d-ff6e-7a22d13a82f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming categorial features...\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "|State|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|MIS_Status|Sector|ApprovalMonth|IsFranchise|clean_DisbursementGross|clean_ChgOffPrinGr|clean_GrAppv|clean_SBA_Appv|StateIndex|      StateVec|BankIndex|           BankVec|BankStateIndex|   BankStateVec|UrbanRuralIndex|UrbanRuralVec|RevLineCrIndex| RevLineCrVec|LowDocIndex|    LowDocVec|SectorIndex|     SectorVec|ApprovalMonthIndex|ApprovalMonthVec|            features|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "|   MA|TD BANK, NATIONAL...|       DE|  84|    2|       0|        0|          2|         1|        1|     0|         1|    44|          Mar|          0|                25959.0|               0.0|     10000.0|        5000.0|       4.0|(50,[4],[1.0])|     11.0| (2945,[11],[1.0])|           9.0| (52,[9],[1.0])|            0.0|(2,[0],[1.0])|           1.0|    (1,[],[])|        0.0|(1,[0],[1.0])|        1.0|(20,[1],[1.0])|               1.0|  (11,[1],[1.0])|(3092,[0,1,4,6,8,...|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    7|       0|        0|          7|         1|        1|     0|         1|    23|          Jun|          0|                98479.0|               0.0|     50000.0|       25000.0|       4.0|(50,[4],[1.0])|      3.0|  (2945,[3],[1.0])|           4.0| (52,[4],[1.0])|            0.0|(2,[0],[1.0])|           1.0|    (1,[],[])|        0.0|(1,[0],[1.0])|        3.0|(20,[3],[1.0])|               4.0|  (11,[4],[1.0])|(3092,[0,1,4,6,8,...|\n",
            "|   MA|FLORENCE SAVINGS ...|       MA|  60|    2|       0|        0|          2|         1|        1|     0|         1|    23|          Apr|          0|               135070.0|               0.0|     35000.0|       17500.0|       4.0|(50,[4],[1.0])|    501.0|(2945,[501],[1.0])|          11.0|(52,[11],[1.0])|            0.0|(2,[0],[1.0])|           1.0|    (1,[],[])|        0.0|(1,[0],[1.0])|        3.0|(20,[3],[1.0])|               2.0|  (11,[2],[1.0])|(3092,[0,1,4,6,8,...|\n",
            "|   MA|CITIZENS BANK NAT...|       RI|  84|    4|       0|        0|          4|         1|        0|     0|         1|    72|          Mar|          0|                20000.0|               0.0|     20000.0|       10000.0|       4.0|(50,[4],[1.0])|      3.0|  (2945,[3],[1.0])|           4.0| (52,[4],[1.0])|            0.0|(2,[0],[1.0])|           0.0|(1,[0],[1.0])|        0.0|(1,[0],[1.0])|        6.0|(20,[6],[1.0])|               1.0|  (11,[1],[1.0])|(3092,[0,1,4,6,8,...|\n",
            "|   MA|BANK OF AMERICA N...|       MA|  84|    6|       1|        0|          0|         0|        0|     1|         1|    81|          Apr|          0|                50000.0|               0.0|     50000.0|       45000.0|       4.0|(50,[4],[1.0])|      0.0|  (2945,[0],[1.0])|          11.0|(52,[11],[1.0])|            1.0|(2,[1],[1.0])|           0.0|(1,[0],[1.0])|        1.0|    (1,[],[])|        4.0|(20,[4],[1.0])|               2.0|  (11,[2],[1.0])|(3092,[0,1,2,6,8,...|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+-----------------------+------------------+------------+--------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Splitting data into training, validation and test...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Transforming categorial features...\")\n",
        "# List of categorical columns to be one-hot encoded\n",
        "categorical_columns = [\"Name\", \"City\", \"State\", \"Zip\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "# ======================================================\n",
        "# dropping these columns give better accuracy (by trial)\n",
        "# ======================================================\n",
        "loan_df = loan_df.drop('Name')\n",
        "categorical_columns = [\"City\", \"State\", \"Zip\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "\n",
        "loan_df = loan_df.drop('Zip')\n",
        "categorical_columns = [\"City\", \"State\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "\n",
        "loan_df = loan_df.drop('City')\n",
        "categorical_columns = [\"State\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "\n",
        "# Define an empty list to store the pipeline stages\n",
        "stages = []\n",
        "\n",
        "# Iterate over each categorical column\n",
        "for column in categorical_columns:\n",
        "    # Define StringIndexer for the current column\n",
        "    indexer = StringIndexer(inputCol=column, outputCol=column + \"Index\")\n",
        "\n",
        "    # Define OneHotEncoder for the indexed column\n",
        "    encoder = OneHotEncoder(inputCol=column + \"Index\", outputCol=column + \"Vec\")\n",
        "\n",
        "    # Add StringIndexer and OneHotEncoder to the list of stages\n",
        "    stages += [indexer, encoder]\n",
        "label_column = \"MIS_Status\"\n",
        "\n",
        "\n",
        "\n",
        "# Create VectorAssembler for combining all features\n",
        "# List of input columns (excluding the label column and categorical columns)\n",
        "input_columns = [col for col in loan_df.columns if col != label_column and col not in categorical_columns]\n",
        "input_columns += [column + \"Vec\" for column in categorical_columns]\n",
        "assembler = VectorAssembler(inputCols=input_columns , outputCol=\"features\")\n",
        "\n",
        "# Combine all stages into a Pipeline\n",
        "pipeline = Pipeline(stages=stages + [assembler])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "pipeline_model = pipeline.fit(loan_df)\n",
        "\n",
        "# Transform your data using the pipeline\n",
        "transformed_data = pipeline_model.transform(loan_df)\n",
        "transformed_data.show(5)\n",
        "print(\"Splitting data into training, validation and test...\")\n",
        "# Split the transformed data into training and test sets (70% training, 30% test)\n",
        "# (trainingData, testData) = transformed_data.randomSplit([0.7, 0.3])\n",
        "(trainingData, validationData, testData) = transformed_data.randomSplit([0.6, 0.2, 0.2], seed=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaOmQXWcArZW"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lQbkPiDcAq5V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training logistic regression model...\n"
          ]
        }
      ],
      "source": [
        "# Create a Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=10, elasticNetParam=0.8, labelCol=label_column, featuresCol=\"features\")\n",
        "print(\"Training logistic regression model...\")\n",
        "# Train the model\n",
        "lrModel = lr.fit(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbBBDrD09AJV",
        "outputId": "cd133cd5-cf2b-4118-a3cc-cca170a4b4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.936868094509943\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lrModel.transform(validationData)\n",
        "\n",
        "# predictions.describe().show()\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=label_column)\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9PwXfyO-wGR",
        "outputId": "63cddfcc-ba4c-422d-c753-fd778acd4fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------------------+--------------------+--------------------+\n",
            "| Model              | Logistic Regression   |                    |                    |\n",
            "+====================+=======================+====================+====================+\n",
            "| Metric             | Training              | Validation         | Test               |\n",
            "+--------------------+-----------------------+--------------------+--------------------+\n",
            "| Accuracy           | 0.9799825643776824    | 0.9694754519411242 | 0.9667760867402765 |\n",
            "+--------------------+-----------------------+--------------------+--------------------+\n",
            "| Weighted Precision | 0.9798398663270723    | 0.9691101660145799 | 0.9663388220602683 |\n",
            "+--------------------+-----------------------+--------------------+--------------------+\n",
            "| Weighted Recall    | 0.9799825643776824    | 0.9694754519411242 | 0.9667760867402766 |\n",
            "+--------------------+-----------------------+--------------------+--------------------+\n",
            "| F1 Score           | 0.9798340511621209    | 0.9691221798955781 | 0.9663307080808471 |\n",
            "+--------------------+-----------------------+--------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(lrModel, trainingData, validationData, testData, 'Logistic Regression', label_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn7ftf81Aj3c"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3TWOfuOx9AJV"
      },
      "outputs": [],
      "source": [
        "\n",
        " # Create Random Forest model\n",
        "rf = RandomForestClassifier(featuresCol='features', labelCol=label_column)\n",
        "\n",
        "# Fit model to training data\n",
        "rf_model = rf.fit(trainingData)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_9hrOjY9AJW",
        "outputId": "12b75941-f2d0-488e-9e20-20223e5c88ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Model              | Random Forest      |                    |                    |\n",
            "+====================+====================+====================+====================+\n",
            "| Metric             | Training           | Validation         | Test               |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Accuracy           | 0.8159871244635193 | 0.821298034179591  | 0.8210484432507709 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Precision | 0.665834987290243  | 0.6745304609472607 | 0.6741205461645143 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Recall    | 0.8159871244635193 | 0.821298034179591  | 0.8210484432507709 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| F1 Score           | 0.7333036433140401 | 0.7407139834212855 | 0.7403653084166562 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "evaluate_model(rf_model, trainingData, validationData, testData, 'Random Forest', label_column)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqCb1jHCf2z"
      },
      "source": [
        "## GBTClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "8_jtI-eN_IfO",
        "outputId": "730ff1c7-fd12-4755-8c84-f01a8a8624b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Evaluating...\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Model              | GBTClassifier      |                    |                    |\n",
            "+====================+====================+====================+====================+\n",
            "| Metric             | Training           | Validation         | Test               |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Accuracy           | 0.9954063841201717 | 0.9940729032895387 | 0.992937431612454  |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Precision | 0.9954698078929716 | 0.9941113474118869 | 0.9929959235953381 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Recall    | 0.9954063841201717 | 0.9940729032895387 | 0.992937431612454  |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| F1 Score           | 0.9954216012061459 | 0.9940856266537357 | 0.9929564427916348 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "# (trainingData, testData) = transformed_data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a GBT model.\n",
        "gbt = GBTClassifier(featuresCol='features', labelCol=label_column, maxIter=100)\n",
        "print(\"Training...\")\n",
        "# Train model.  This also runs the indexers.\n",
        "gbt_model = gbt.fit(trainingData)\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "evaluate_model(gbt_model, trainingData, validationData, testData, 'GBTClassifier', label_column)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePzA52kxDpVt"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUmXa11LJ2Cb",
        "outputId": "16aa341d-1ba7-459d-8796-f8ec275cae9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Evaluating...\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Model              | SVM                |                    |                    |\n",
            "+====================+====================+====================+====================+\n",
            "| Metric             | Training           | Validation         | Test               |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Accuracy           | 0.8817730686695279 | 0.8781981626000197 | 0.8750621704963693 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Precision | 0.8853828220351001 | 0.8795728016375877 | 0.8762376485083178 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| Weighted Recall    | 0.8817730686695279 | 0.8781981626000197 | 0.8750621704963693 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "| F1 Score           | 0.8618950261167185 | 0.8562748789789716 | 0.8516520115168842 |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "lsvc = LinearSVC(featuresCol='features', labelCol=label_column,maxIter=100, regParam=0.1)\n",
        "print(\"Training...\")\n",
        "# Fit the model\n",
        "lsvcModel = lsvc.fit(trainingData)\n",
        "print(\"Evaluating...\")\n",
        "evaluate_model(lsvcModel, trainingData, validationData, testData, 'SVM', label_column)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BiirzAbEJds"
      },
      "source": [
        "## Kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y0esreNkJ3cr",
        "outputId": "9452ef7f-eacd-4772-a0b4-2e9c788adba9"
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Column MIS_Status already exists.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Trains a k-means model.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39mlabel_column)\u001b[38;5;241m.\u001b[39msetK(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msetSeed(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(validationData)\n",
            "File \u001b[1;32mc:\\Users\\bemoi\\miniconda3\\envs\\v38_env\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\bemoi\\miniconda3\\envs\\v38_env\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[1;32mc:\\Users\\bemoi\\miniconda3\\envs\\v38_env\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bemoi\\miniconda3\\envs\\v38_env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\bemoi\\miniconda3\\envs\\v38_env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column MIS_Status already exists."
          ]
        }
      ],
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol='features', predictionCol=label_column).setK(2).setSeed(1)\n",
        "model = kmeans.fit(trainingData)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(validationData)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"(validationData) Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "# Make predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"(testData) Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27pMFGgt9AJW"
      },
      "source": [
        " ## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEr65Bs79AJW"
      },
      "outputs": [],
      "source": [
        "# model_path = \"lrModel\"\n",
        "# lrModel.save(model_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MaOmQXWcArZW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "v38_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
