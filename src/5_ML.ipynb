{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chbic3fo9AJR"
      },
      "source": [
        " ## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6Ayhfq9BvN",
        "outputId": "8b94f648-0efa-40aa-d8c9-c3e1c20d931e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rfpbvgavADeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7LveuaXi9AJS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import col, countDistinct, isnan, when, count, round, substring_index,substring, split, regexp_replace, udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, VectorIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator,ClusteringEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, LinearSVC,GBTClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALGQJI-b9QTx",
        "outputId": "a27f33ee-1260-4aa9-c4f8-13af0012183c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "wcITIKfJAJ9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data, model_name , date_type):\n",
        "\n",
        "    # prdict on data\n",
        "    predictions = model.transform(data)\n",
        "\n",
        "    # Create evaluators for different metrics\n",
        "    evaluator_multi = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='accuracy')\n",
        "    evaluator_weighted_precision = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='weightedPrecision')\n",
        "    evaluator_weighted_recall = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='weightedRecall')\n",
        "    evaluator_f1 = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol=label_column, metricName='f1')\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = evaluator_multi.evaluate(predictions)\n",
        "    weighted_precision = evaluator_weighted_precision.evaluate(predictions)\n",
        "    weighted_recall = evaluator_weighted_recall.evaluate(predictions)\n",
        "    f1 = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "    # Print results\n",
        "    print('-------------------------------------------------------------------------------------------------------------------')\n",
        "    print(f'---------------------------------------------- Model: {model_name} -----------------------------------------------')\n",
        "    print('-------------------------------------------------------------------------------------------------------------------')\n",
        "    print(f'Data Type: {date_type}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Weighted Precision: {weighted_precision}')\n",
        "    print(f'Weighted Recall: {weighted_recall}')\n",
        "    print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "FtFnxOG2AMkv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OcvZDw5A9AJT"
      },
      "outputs": [],
      "source": [
        "\n",
        "spark=SparkSession.builder\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"LoanApproval\")\\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xxihZQnF9AJT"
      },
      "outputs": [],
      "source": [
        "\n",
        "sc=spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2HnF7W69AJU"
      },
      "source": [
        " ## Read Data - SBAnational.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1HbGwdzc9AJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_path=\"/content/drive/MyDrive/Colab Notebooks/BD_project/50000.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JjluWYjC9AJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "loan_df =  spark.read.csv(data_path, header=True, inferSchema=True, multiLine=True, quote='\"', escape='\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkDD_RBf9AJU",
        "outputId": "b8665171-c592-4229-a752-3348289ba566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Zip: integer (nullable = true)\n",
            " |-- Bank: string (nullable = true)\n",
            " |-- BankState: string (nullable = true)\n",
            " |-- Term: integer (nullable = true)\n",
            " |-- NoEmp: integer (nullable = true)\n",
            " |-- NewExist: integer (nullable = true)\n",
            " |-- CreateJob: integer (nullable = true)\n",
            " |-- RetainedJob: integer (nullable = true)\n",
            " |-- UrbanRural: integer (nullable = true)\n",
            " |-- RevLineCr: integer (nullable = true)\n",
            " |-- LowDoc: integer (nullable = true)\n",
            " |-- MIS_Status: integer (nullable = true)\n",
            " |-- Sector: integer (nullable = true)\n",
            " |-- ApprovalMonth: string (nullable = true)\n",
            " |-- IsFranchise: integer (nullable = true)\n",
            " |-- clean_GrAppv: double (nullable = true)\n",
            "\n",
            "+--------------------+-----------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+\n",
            "|                Name|       City|State| Zip|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|MIS_Status|Sector|ApprovalMonth|IsFranchise|clean_GrAppv|\n",
            "+--------------------+-----------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+\n",
            "|Chartier's Genera...|     AGAWAM|   MA|1001|         UNITED BANK|       MA|  37|    4|       0|        1|          4|         2|        0|     0|         0|    23|          May|          0|     75000.0|\n",
            "|LIEBMANN OPTICAL ...|EASTHAMPTON|   MA|1027|NEW ENGLAND CERT....|       MA| 240|   21|       0|        7|         14|         0|        0|     0|         1|    31|          Apr|          0|    257000.0|\n",
            "|RELIEF RESOURCES INC|     HADLEY|   MA|1035|BANK OF WESTERN M...|       MA|  60|  318|       0|        0|        318|         1|        1|     0|         1|    56|          Jun|          0|    150000.0|\n",
            "|KEN WARREN ASSOCI...| PITTSFIELD|   MA|1201|      BERKSHIRE BANK|       MA| 120|   12|       0|        0|          0|         0|        0|     0|         1|     0|          Oct|          0|    115500.0|\n",
            "|JENNY'S BREAKFAST...|   HOPEDALE|   MA|1247|MILFORD NATL BK &...|       MA|  84|    2|       1|        0|          0|         0|        0|     1|         1|     0|          Apr|          0|     37000.0|\n",
            "+--------------------+-----------+-----+----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loan_df.printSchema()\n",
        "loan_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPp_5zgk9AJU",
        "outputId": "b20f8619-2703-4c4d-ff6e-7a22d13a82f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transforming categorial features...\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "|State|                Bank|BankState|Term|NoEmp|NewExist|CreateJob|RetainedJob|UrbanRural|RevLineCr|LowDoc|MIS_Status|Sector|ApprovalMonth|IsFranchise|clean_GrAppv|StateIndex|      StateVec|BankIndex|           BankVec|BankStateIndex|   BankStateVec|UrbanRuralIndex|UrbanRuralVec|RevLineCrIndex| RevLineCrVec|LowDocIndex|    LowDocVec|SectorIndex|     SectorVec|ApprovalMonthIndex|ApprovalMonthVec|            features|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "|   MA|         UNITED BANK|       MA|  37|    4|       0|        1|          4|         2|        0|     0|         0|    23|          May|          0|     75000.0|       4.0|(37,[4],[1.0])|     38.0| (1934,[38],[1.0])|          10.0|(48,[10],[1.0])|            2.0|    (2,[],[])|           0.0|(1,[0],[1.0])|        0.0|(1,[0],[1.0])|        4.0|(20,[4],[1.0])|               5.0|  (11,[5],[1.0])|(2061,[0,1,3,4,6,...|\n",
            "|   MA|NEW ENGLAND CERT....|       MA| 240|   21|       0|        7|         14|         0|        0|     0|         1|    31|          Apr|          0|    257000.0|       4.0|(37,[4],[1.0])|    107.0|(1934,[107],[1.0])|          10.0|(48,[10],[1.0])|            1.0|(2,[1],[1.0])|           0.0|(1,[0],[1.0])|        0.0|(1,[0],[1.0])|        5.0|(20,[5],[1.0])|               1.0|  (11,[1],[1.0])|(2061,[0,1,3,4,6,...|\n",
            "|   MA|BANK OF WESTERN M...|       MA|  60|  318|       0|        0|        318|         1|        1|     0|         1|    56|          Jun|          0|    150000.0|       4.0|(37,[4],[1.0])|    150.0|(1934,[150],[1.0])|          10.0|(48,[10],[1.0])|            0.0|(2,[0],[1.0])|           1.0|    (1,[],[])|        0.0|(1,[0],[1.0])|        9.0|(20,[9],[1.0])|               6.0|  (11,[6],[1.0])|(2061,[0,1,4,6,11...|\n",
            "|   MA|      BERKSHIRE BANK|       MA| 120|   12|       0|        0|          0|         0|        0|     0|         1|     0|          Oct|          0|    115500.0|       4.0|(37,[4],[1.0])|     72.0| (1934,[72],[1.0])|          10.0|(48,[10],[1.0])|            1.0|(2,[1],[1.0])|           0.0|(1,[0],[1.0])|        0.0|(1,[0],[1.0])|        0.0|(20,[0],[1.0])|               7.0|  (11,[7],[1.0])|(2061,[0,1,6,11,1...|\n",
            "|   MA|MILFORD NATL BK &...|       MA|  84|    2|       1|        0|          0|         0|        0|     1|         1|     0|          Apr|          0|     37000.0|       4.0|(37,[4],[1.0])|    521.0|(1934,[521],[1.0])|          10.0|(48,[10],[1.0])|            1.0|(2,[1],[1.0])|           0.0|(1,[0],[1.0])|        1.0|    (1,[],[])|        0.0|(20,[0],[1.0])|               1.0|  (11,[1],[1.0])|(2061,[0,1,2,6,11...|\n",
            "+-----+--------------------+---------+----+-----+--------+---------+-----------+----------+---------+------+----------+------+-------------+-----------+------------+----------+--------------+---------+------------------+--------------+---------------+---------------+-------------+--------------+-------------+-----------+-------------+-----------+--------------+------------------+----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Splitting data into training, validation and test...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Transforming categorial features...\")\n",
        "# List of categorical columns to be one-hot encoded\n",
        "categorical_columns = [\"Name\", \"City\", \"State\", \"Zip\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "# ======================================================\n",
        "# dropping these columns give better accuracy (by trial)\n",
        "# ======================================================\n",
        "loan_df = loan_df.drop('Name')\n",
        "categorical_columns = [\"City\", \"State\", \"Zip\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "\n",
        "loan_df = loan_df.drop('Zip')\n",
        "categorical_columns = [\"City\", \"State\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "\n",
        "loan_df = loan_df.drop('City')\n",
        "categorical_columns = [\"State\", \"Bank\", \"BankState\", \"UrbanRural\", \"RevLineCr\", \"LowDoc\", \"Sector\", \"ApprovalMonth\"]\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "# ======================================================\n",
        "\n",
        "# Define an empty list to store the pipeline stages\n",
        "stages = []\n",
        "\n",
        "# Iterate over each categorical column\n",
        "for column in categorical_columns:\n",
        "    # Define StringIndexer for the current column\n",
        "    indexer = StringIndexer(inputCol=column, outputCol=column + \"Index\")\n",
        "\n",
        "    # Define OneHotEncoder for the indexed column\n",
        "    encoder = OneHotEncoder(inputCol=column + \"Index\", outputCol=column + \"Vec\")\n",
        "\n",
        "    # Add StringIndexer and OneHotEncoder to the list of stages\n",
        "    stages += [indexer, encoder]\n",
        "label_column = \"MIS_Status\"\n",
        "\n",
        "\n",
        "\n",
        "# Create VectorAssembler for combining all features\n",
        "# List of input columns (excluding the label column and categorical columns)\n",
        "input_columns = [col for col in loan_df.columns if col != label_column and col not in categorical_columns]\n",
        "input_columns += [column + \"Vec\" for column in categorical_columns]\n",
        "assembler = VectorAssembler(inputCols=input_columns , outputCol=\"features\")\n",
        "\n",
        "# Combine all stages into a Pipeline\n",
        "pipeline = Pipeline(stages=stages + [assembler])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "pipeline_model = pipeline.fit(loan_df)\n",
        "\n",
        "# Transform your data using the pipeline\n",
        "transformed_data = pipeline_model.transform(loan_df)\n",
        "transformed_data.show(5)\n",
        "print(\"Splitting data into training, validation and test...\")\n",
        "# Split the transformed data into training and test sets (70% training, 30% test)\n",
        "# (trainingData, testData) = transformed_data.randomSplit([0.7, 0.3])\n",
        "(trainingData, validationData, testData) = transformed_data.randomSplit([0.6, 0.2, 0.2], seed=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "MaOmQXWcArZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=10, elasticNetParam=0.8, labelCol=label_column, featuresCol=\"features\")\n",
        "print(\"Training logistic regression model...\")\n",
        "# Train the model\n",
        "lrModel = lr.fit(trainingData)"
      ],
      "metadata": {
        "id": "lQbkPiDcAq5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbBBDrD09AJV",
        "outputId": "cd133cd5-cf2b-4118-a3cc-cca170a4b4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7396289805425709\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make predictions on the test data\n",
        "predictions = lrModel.transform(validationData)\n",
        "\n",
        "# predictions.describe().show()\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=label_column)\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(lrModel, trainingData, 'Logistic Regression', 'train')\n",
        "evaluate_model(lrModel, validationData, 'Logistic Regression', 'validation')\n",
        "evaluate_model(lrModel, testData, 'Logistic Regression', 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9PwXfyO-wGR",
        "outputId": "63cddfcc-ba4c-422d-c753-fd778acd4fd5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Logistic Regression -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: train\n",
            "Accuracy: 0.8892502682403434\n",
            "Weighted Precision: 0.8823189751461389\n",
            "Weighted Recall: 0.8892502682403433\n",
            "F1 Score: 0.8817209438871593\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Logistic Regression -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: validation\n",
            "Accuracy: 0.8795811518324608\n",
            "Weighted Precision: 0.8708062421696577\n",
            "Weighted Recall: 0.8795811518324608\n",
            "F1 Score: 0.8713103143983274\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Logistic Regression -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: test\n",
            "Accuracy: 0.8763553168208496\n",
            "Weighted Precision: 0.8673955727747407\n",
            "Weighted Recall: 0.8763553168208496\n",
            "F1 Score: 0.8687630827948416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "Qn7ftf81Aj3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3TWOfuOx9AJV"
      },
      "outputs": [],
      "source": [
        "\n",
        " # Create Random Forest model\n",
        "rf = RandomForestClassifier(featuresCol='features', labelCol=label_column)\n",
        "\n",
        "# Fit model to training data\n",
        "rf_model = rf.fit(trainingData)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_9hrOjY9AJW",
        "outputId": "12b75941-f2d0-488e-9e20-20223e5c88ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Random Forest -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: train\n",
            "Accuracy: 0.818468347639485\n",
            "Weighted Precision: 0.6698904360877088\n",
            "Weighted Recall: 0.818468347639485\n",
            "F1 Score: 0.7367633722712625\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Random Forest -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: validation\n",
            "Accuracy: 0.8203101847278474\n",
            "Weighted Precision: 0.6729087991682352\n",
            "Weighted Recall: 0.8203101847278474\n",
            "F1 Score: 0.7393342132718342\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: Random Forest -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: test\n",
            "Accuracy: 0.8203521336914354\n",
            "Weighted Precision: 0.6729776232520908\n",
            "Weighted Recall: 0.8203521336914354\n",
            "F1 Score: 0.7393927919730348\n"
          ]
        }
      ],
      "source": [
        "\n",
        "evaluate_model(rf_model, trainingData, 'Random Forest', 'train')\n",
        "evaluate_model(rf_model, validationData, 'Random Forest', 'validation')\n",
        "evaluate_model(rf_model, testData, 'Random Forest', 'test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GBTClassifier"
      ],
      "metadata": {
        "id": "QLqCb1jHCf2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "# (trainingData, testData) = transformed_data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a GBT model.\n",
        "gbt = GBTClassifier(featuresCol='features', labelCol=label_column, maxIter=1000)\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "gbt_model = gbt.fit(trainingData)\n",
        "\n",
        "evaluate_model(gbt_model, trainingData, 'GBTClassifier', 'train')\n",
        "evaluate_model(gbt_model, validationData, 'GBTClassifier', 'validation')\n",
        "evaluate_model(gbt_model, testData, 'GBTClassifier', 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "8_jtI-eN_IfO",
        "outputId": "730ff1c7-fd12-4755-8c84-f01a8a8624b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "py4j does not exist in the JVM",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-667ee4b930d3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train model.  This also runs the indexers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GBTClassifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPythonException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptionString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mgw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_instance_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.catalyst.parser.ParseException\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Order matters. ParseException inherits AnalysisException.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \"java_class must be a string, a JavaClass, or a JavaObject\")\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n\u001b[0m\u001b[1;32m    465\u001b[0m         param, java_object)\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1724\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "ePzA52kxDpVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvc = LinearSVC(featuresCol='features', labelCol=label_column,maxIter=100, regParam=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lsvcModel = lsvc.fit(trainingData)\n",
        "\n",
        "evaluate_model(lsvcModel, trainingData, 'SVM', 'train')\n",
        "evaluate_model(lsvcModel, validationData, 'SVM', 'validation')\n",
        "evaluate_model(lsvcModel, testData, 'SVM', 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUmXa11LJ2Cb",
        "outputId": "16aa341d-1ba7-459d-8796-f8ec275cae9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function JavaWrapper.__del__ at 0x7db5203020e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\", line 53, in __del__\n",
            "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
            "AttributeError: 'KMeans' object has no attribute '_java_obj'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: SVM -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: train\n",
            "Accuracy: 0.8309750536480687\n",
            "Weighted Precision: 0.8150335588823321\n",
            "Weighted Recall: 0.8309750536480687\n",
            "F1 Score: 0.7757206985547561\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: SVM -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: validation\n",
            "Accuracy: 0.8281141953966216\n",
            "Weighted Precision: 0.8003421873974659\n",
            "Weighted Recall: 0.8281141953966216\n",
            "F1 Score: 0.7709744904088325\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------- Model: SVM -----------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Data Type: test\n",
            "Accuracy: 0.826221028548692\n",
            "Weighted Precision: 0.7923876282652156\n",
            "Weighted Recall: 0.826221028548692\n",
            "F1 Score: 0.768992136368534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kmeans"
      ],
      "metadata": {
        "id": "3BiirzAbEJds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol='features', predictionCol=label_column).setK(2).setSeed(1)\n",
        "model = kmeans.fit(trainingData)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(validationData)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"(validationData) Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "# Make predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"(testData) Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y0esreNkJ3cr",
        "outputId": "9452ef7f-eacd-4772-a0b4-2e9c788adba9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Column MIS_Status already exists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-320ec7da3eaa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trains a k-means model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column MIS_Status already exists."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27pMFGgt9AJW"
      },
      "source": [
        " ## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YEr65Bs79AJW"
      },
      "outputs": [],
      "source": [
        "# model_path = \"lrModel\"\n",
        "# lrModel.save(model_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "v38_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MaOmQXWcArZW"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}